# Between Artificial and Human Cognition - Final Project

## Overview
This project explores the presence of acquiescence bias in responses generated by various language models. Acquiescence bias, or the tendency to agree regardless of the content, can significantly distort data collection in research studies. Our objective was to assess and potentially mitigate this bias in AI models used for automated survey responses.

## Methodology
We utilized a "Statement Test" benchmark method, which presents a set of 100 neutral, opinion-based statements to language models. Responses were analyzed using the Chi-squared test to determine the deviation from a balanced (bias-free) response distribution.

### Models Tested
- **GPT-2**
- **Gemma 2b**
- **Phi 2**

Each model was provided with statements, and their agreement or disagreement was recorded. Post-analysis, attempts were made to de-bias the models using the "Chain of Thoughts" (CoT) method, which introduces a reasoning step in the model's decision process.

## Key Findings
- **GPT-2** exhibited a strong disagreement bias, suggesting a tendency to disagree regardless of the statement content.
- **Gemma 2b** showed signs of acquiescence bias, though it was the most balanced among the models.
- **Phi 2** demonstrated a consistent acquiescence bias, agreeing with every statement presented.

### De-Biasing Attempts
- The CoT method improved the response distribution of GPT-2 and effectively de-biased Gemma 2b. However, it had no significant effect on Phi 2.

## Conclusion
The study underscores the challenges and potential strategies for mitigating bias in AI-generated survey responses. While some methods showed promise, the effectiveness varied significantly across different models.

## Technologies Used
- Python
- PyTorch
- Transformers library
- SciPy

## Note
Details and specific results of this research are kept confidential due to academic privacy concerns. This summary reflects the general scope and findings without disclosing sensitive data.

